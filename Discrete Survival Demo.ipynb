{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set backend for Keras to be Theano\n",
    "# You can make multiple Keras models this way w/o slowing down, unlike Tensorflow\n",
    "\n",
    "from keras import backend as K\n",
    "import os\n",
    "from importlib import reload\n",
    "\n",
    "def set_keras_backend(backend):\n",
    "\n",
    "    if K.backend() != backend:\n",
    "        os.environ['KERAS_BACKEND'] = backend\n",
    "        reload(K)\n",
    "        assert K.backend() == backend\n",
    "\n",
    "set_keras_backend(\"theano\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd ~/nnet-survival/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "from keras.models import Sequential, Model\n",
    "from keras import optimizers, layers, regularizers\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.layers import Input, Dense, Activation\n",
    "from keras.models import load_model\n",
    "from lifelines import KaplanMeierFitter\n",
    "from lifelines import CoxPHFitter\n",
    "from lifelines.utils import concordance_index\n",
    "import nnet_survival"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation, Dropout\n",
    "from keras.optimizers import SGD, RMSprop\n",
    "import theano.tensor as T\n",
    "\n",
    "from lifelines.utils import concordance_index\n",
    "from lifelines import CoxPHFitter\n",
    "\n",
    "from lifelines.datasets import load_rossi\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get disease codes for each disease type that we want to make comparisons between baseline/transfer\n",
    "\n",
    "df2 = pd.read_csv('/home/nolelin/clinical3.csv')\n",
    "df2 = df2.loc[df2['name'] == 'disease_code']\n",
    "disease_codes = list(set(list(df2['value'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all case_ids of patients for each disease type\n",
    "\n",
    "disease_caseids = {}\n",
    "df2 = pd.read_csv('/home/nolelin/clinical3.csv')\n",
    "for disease in disease_codes:\n",
    "    diseasedf = df2.loc[df2['value'] == disease]\n",
    "    disease_caseids[disease] = list(diseasedf['case_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only train/validate on 80% of all patients\n",
    "# Do 10 runs\n",
    "\n",
    "from random import shuffle\n",
    "\n",
    "runs_constrained_caseids = {}\n",
    "for run in range(10):\n",
    "    constrained_caseids = []\n",
    "    for disease in disease_caseids.keys():\n",
    "        num_patients = len(disease_caseids[disease])\n",
    "        shuffle(disease_caseids[disease])\n",
    "        constrained_caseids.extend(disease_caseids[disease][0:int(0.8 * num_patients)])\n",
    "    runs_constrained_caseids[run] = constrained_caseids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get best mutation feature indices\n",
    "\n",
    "best_mut_feat_dict = np.load('/home/nolelin/best_mut_feats.npy', encoding='latin1').item()\n",
    "best_mut_feats = sorted(best_mut_feat_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean and preprocess data for global model\n",
    "\n",
    "import numpy as np\n",
    "import sys\n",
    "import numpy.ma as ma\n",
    "from sklearn import preprocessing\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "runs_x_train = {}\n",
    "runs_x_test = {}\n",
    "runs_x_val = {}\n",
    "runs_y_train = {}\n",
    "runs_y_test = {}\n",
    "runs_y_val = {}\n",
    "runs_e_train = {}\n",
    "runs_e_test = {}\n",
    "runs_e_val = {}\n",
    "runs_n_intervals = {}\n",
    "runs_breaks = {}\n",
    "    \n",
    "for run in range(10):\n",
    "    print(run)\n",
    "    disease_dfs = []\n",
    "    for disease in disease_codes:\n",
    "        if disease == 'FPPP':\n",
    "            continue\n",
    "        df = pd.read_csv('/mnt/raid/cpg_fs_checkpoints/meth_mut_exp_features/df.csv')[['features', 'days_to_death', 'days_to_followup', 'case_id', 'mut_features', 'days_to_birth', 'exp_features']]\n",
    "        df = df.dropna(subset=['days_to_death', 'days_to_followup'], how='all')\n",
    "        df = df.loc[df['case_id'].isin(disease_caseids[disease])]\n",
    "        df['disease_code'] = disease\n",
    "        disease_dfs.append(df)\n",
    "    df = pd.concat(disease_dfs)\n",
    "    df = pd.get_dummies(df, columns=['disease_code'])\n",
    "    disease_cols = [x for x in df.columns if 'disease_code' in x]\n",
    "    \n",
    "    output_cols = ['days_to_death', 'days_to_followup']\n",
    "    imp = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "    x = np.asarray([[float(y) if y != 'NA' else np.nan for y in x[1:-1].split(\", \")] for x in list(df['features'])])\n",
    "    x_mut = np.asarray([[float(y) if y != 'NA' else np.nan for y in x[1:-1].split(\", \")] for x in list(df['mut_features'])])\n",
    "    x_exp = np.asarray([[float(y) if y != 'NA' else np.nan for y in x[1:-1].split(\", \")] for x in list(df['exp_features'])])\n",
    "    mut_load = np.sum(x_mut, axis=1).reshape(df.shape[0], 1)\n",
    "    x = np.hstack((x, mut_load))\n",
    "    x = np.hstack((x, np.expand_dims(np.asarray(df['days_to_birth']), axis=1)))\n",
    "    x = np.hstack((x, x_exp))\n",
    "    x = np.where(np.isnan(x), ma.array(x, mask=np.isnan(x)).mean(axis=0), x)\n",
    "    imp.fit(x)\n",
    "    x = imp.transform(x)\n",
    "    scaler = preprocessing.StandardScaler().fit(x)\n",
    "    days = np.asarray(df[output_cols])\n",
    "    y_whole = np.asarray([x[0] if np.isnan(x[0]) == False else x[1] for x in days])\n",
    "    breaks=np.arange(min(y_whole), max(y_whole), 200)\n",
    "    n_intervals=len(breaks)-1\n",
    "    \n",
    "    test = df.loc[~df['case_id'].isin(runs_constrained_caseids[run])]\n",
    "    df = df.loc[df['case_id'].isin(runs_constrained_caseids[run])]\n",
    "    df = df.sample(frac=1)\n",
    "    train_size = int(0.75 * df.shape[0])\n",
    "    test_size = df.shape[0] - train_size\n",
    "    train = df.head(train_size)\n",
    "    validate = df.tail(test_size)\n",
    "    dtb_train = np.expand_dims(np.asarray(train['days_to_birth']), axis=1)\n",
    "    mut_train = np.asarray([[float(y) if y != 'NA' else np.nan for y in x[1:-1].split(\", \")] for x in list(train['mut_features'])])\n",
    "    mut_train_load = np.sum(mut_train, axis=1).reshape(train.shape[0], 1)\n",
    "    mut_train = mut_train[:, best_mut_feats]\n",
    "    exp_train = np.asarray([[float(y) if y != 'NA' else np.nan for y in x[1:-1].split(\", \")] for x in list(train['exp_features'])])\n",
    "    x_train = np.asarray([[float(y) if y != 'NA' else np.nan for y in x[1:-1].split(\", \")] for x in list(train['features'])])\n",
    "    x_train = np.hstack((x_train, mut_train_load))\n",
    "    x_train = np.hstack((x_train, dtb_train))\n",
    "    x_train = np.hstack((x_train, exp_train))\n",
    "    x_train = imp.transform(x_train)\n",
    "    x_train = scaler.transform(x_train)\n",
    "    x_train = x_train[:, ~np.all(x_train == 0, axis=0)]\n",
    "    x_train = np.hstack((x_train, mut_train))\n",
    "    x_train = np.hstack((x_train, train[disease_cols]))\n",
    "    days = np.asarray(train[output_cols])\n",
    "    y_train = np.asarray([x[0] if np.isnan(x[0]) == False else x[1] for x in days])\n",
    "    e_train = np.asarray([1 if np.isnan(x[0]) == False else 0 for x in days])\n",
    "    y_train=nnet_survival.make_surv_array(y_train,[True if x == 1 else False for x in e_train],breaks)\n",
    "        \n",
    "    dtb_test = np.expand_dims(np.asarray(test['days_to_birth']), axis=1)\n",
    "    mut_test = np.asarray([[float(y) if y != 'NA' else np.nan for y in x[1:-1].split(\", \")] for x in list(test['mut_features'])])\n",
    "    mut_test_load = np.sum(mut_test, axis=1).reshape(test.shape[0], 1)\n",
    "    mut_test = mut_test[:, best_mut_feats]\n",
    "    exp_test = np.asarray([[float(y) if y != 'NA' else np.nan for y in x[1:-1].split(\", \")] for x in list(test['exp_features'])])\n",
    "    x_test = np.asarray([[float(y) if y != 'NA' else np.nan for y in x[1:-1].split(\", \")] for x in list(test['features'])])\n",
    "    x_test = np.hstack((x_test, mut_test_load))\n",
    "    x_test = np.hstack((x_test, dtb_test))\n",
    "    x_test = np.hstack((x_test, exp_test))\n",
    "    x_test = imp.transform(x_test)\n",
    "    x_test = scaler.transform(x_test)\n",
    "    x_test = x_test[:, ~np.all(x_test == 0, axis=0)]\n",
    "    x_test = np.hstack((x_test, mut_test))\n",
    "    x_test = np.hstack((x_test, test[disease_cols]))\n",
    "    days = np.asarray(test[output_cols])\n",
    "    y_test = np.asarray([x[0] if np.isnan(x[0]) == False else x[1] for x in days])\n",
    "    e_test = np.asarray([1 if np.isnan(x[0]) == False else 0 for x in days])\n",
    "    y_test=nnet_survival.make_surv_array(y_test,[True if x == 1 else False for x in e_test],breaks)\n",
    "    \n",
    "    dtb_val = np.expand_dims(np.asarray(validate['days_to_birth']), axis=1)\n",
    "    mut_val = np.asarray([[float(y) if y != 'NA' else np.nan for y in x[1:-1].split(\", \")] for x in list(validate['mut_features'])])\n",
    "    mut_val_load = np.sum(mut_val, axis=1).reshape(validate.shape[0], 1)\n",
    "    mut_val = mut_val[:, best_mut_feats]\n",
    "    exp_val = np.asarray([[float(y) if y != 'NA' else np.nan for y in x[1:-1].split(\", \")] for x in list(validate['exp_features'])])\n",
    "    x_val = np.asarray([[float(y) if y != 'NA' else np.nan for y in x[1:-1].split(\", \")] for x in list(validate['features'])])\n",
    "    x_val = np.hstack((x_val, mut_val_load))\n",
    "    x_val = np.hstack((x_val, dtb_val))\n",
    "    x_val = np.hstack((x_val, exp_val))\n",
    "    x_val = imp.transform(x_val)\n",
    "    x_val = scaler.transform(x_val)\n",
    "    x_val = x_val[:, ~np.all(x_val == 0, axis=0)]\n",
    "    x_val = np.hstack((x_val, mut_val))\n",
    "    x_val = np.hstack((x_val, validate[disease_cols]))\n",
    "    days = np.asarray(validate[output_cols])\n",
    "    y_val = np.asarray([x[0] if np.isnan(x[0]) == False else x[1] for x in days])\n",
    "    e_val = np.asarray([1 if np.isnan(x[0]) == False else 0 for x in days])\n",
    "    y_val=nnet_survival.make_surv_array(y_val,[True if x == 1 else False for x in e_val],breaks)\n",
    "    \n",
    "    print(x_train.shape, x_val.shape, x_test.shape)\n",
    "        \n",
    "    if x_train.shape[1] != x_val.shape[1] or x_train.shape[1] != x_test.shape[1] or x_val.shape[1] != x_test.shape[1]:\n",
    "        print(\"mismatched shape\")\n",
    "        continue\n",
    "\n",
    "    if np.sum(e_train) < 0.2 * len(e_train) or np.sum(e_val) < 0.2 * len(e_val) or np.sum(e_test) < 0.2 * len(e_test) or len(e_train) < 150:\n",
    "        print(\"skipping \", disease)\n",
    "        continue\n",
    "    \n",
    "    runs_x_train[run] = x_train\n",
    "    runs_x_test[run] = x_test\n",
    "    runs_x_val[run] = x_val\n",
    "    runs_y_train[run] = y_train\n",
    "    runs_y_test[run] = y_test\n",
    "    runs_y_val[run] = y_val\n",
    "    runs_e_train[run] = e_train\n",
    "    runs_e_test[run] = e_test\n",
    "    runs_e_val[run] = e_val\n",
    "    runs_n_intervals[run] = n_intervals\n",
    "    runs_breaks[run] = breaks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tuning for global models\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Masking\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Flatten\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras import optimizers\n",
    "from keras import regularizers\n",
    "import sys\n",
    "\n",
    "runs_global_errors = {}\n",
    "units = [10, 50, 100, 200, 500, 1000]\n",
    "learning_rates = [0.1, 0.01, 0.001, 0.0001, 0.00001]\n",
    "for run in range(10):\n",
    "    for unit in units:\n",
    "        for learning_rate in learning_rates:\n",
    "            model = Sequential()\n",
    "            model.add(Dense(unit, activation='relu', input_shape=(runs_x_train[run].shape[1],)))\n",
    "            model.add(Dense(runs_n_intervals[run], activation='sigmoid'))\n",
    "            adam = optimizers.Adam(lr=learning_rate)\n",
    "            model.compile(loss=nnet_survival.surv_likelihood(runs_n_intervals[run]), optimizer=adam)\n",
    "            early_stopping = EarlyStopping(monitor='val_loss', patience=2)\n",
    "            history = model.fit(runs_x_train[run], runs_y_train[run], batch_size=256, epochs=1000, shuffle=True, callbacks=[early_stopping], validation_data=(runs_x_val[run], runs_y_val[run]))\n",
    "            best_epochs = history.history['val_loss'].index(min(history.history['val_loss'])) + 1\n",
    "            runs_global_errors[(run, (learning_rate, best_epochs, unit))] = min(history.history['val_loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train global models\n",
    "\n",
    "runs_global_models = {}\n",
    "for run in range(10):\n",
    "    this_runs = [x for x in runs_global_errors.keys() if run == x[0]]\n",
    "    lowest_loss = 10000\n",
    "    best_run = None\n",
    "    for this_run in this_runs:\n",
    "        if runs_global_errors[this_run] < lowest_loss:\n",
    "            lowest_loss = runs_global_errors[this_run]\n",
    "            best_run = this_run\n",
    "    print(best_run, runs_global_errors[best_run])\n",
    "    total_x = np.vstack((runs_x_train[run], runs_x_val[run]))\n",
    "    total_y = np.vstack((runs_y_train[run], runs_y_val[run]))\n",
    "    total_e = np.hstack((runs_e_train[run], runs_e_val[run]))\n",
    "    model = Sequential()\n",
    "    model.add(Dense(best_run[1][2], activation='relu', input_shape=(total_x.shape[1],)))\n",
    "    model.add(Dense(runs_n_intervals[run], activation='sigmoid'))\n",
    "    adam = optimizers.Adam(lr=best_run[1][0])\n",
    "    model.compile(loss=nnet_survival.surv_likelihood(runs_n_intervals[run]), optimizer=adam)\n",
    "    model.fit(total_x, total_y, batch_size=256, epochs=best_run[1][1], shuffle=True)\n",
    "    runs_global_models[run] = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do same data pre-processing for each individual disease\n",
    "# Don't include diseases that don't have at least 150 patients in training set and at least 30% of patients have died\n",
    "\n",
    "import numpy as np\n",
    "import sys\n",
    "import numpy.ma as ma\n",
    "from sklearn import preprocessing\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "runs_disease_x_train = {}\n",
    "runs_disease_x_test = {}\n",
    "runs_disease_x_val = {}\n",
    "runs_disease_y_train = {}\n",
    "runs_disease_y_test = {}\n",
    "runs_disease_y_val = {}\n",
    "runs_disease_y_test_orig = {}\n",
    "runs_disease_e_train = {}\n",
    "runs_disease_e_test = {}\n",
    "runs_disease_e_val = {}\n",
    "runs_disease_n_intervals = {}\n",
    "\n",
    "for run in range(10):\n",
    "    breaks = runs_breaks[run]\n",
    "    n_intervals=len(breaks)-1\n",
    "    for disease in disease_codes:\n",
    "        if disease == 'FPPP':\n",
    "            continue\n",
    "        df = pd.read_csv('/mnt/raid/cpg_fs_checkpoints/meth_mut_exp_features/df.csv')[['features', 'days_to_death', 'days_to_followup', 'case_id', 'mut_features', 'days_to_birth', 'exp_features']]\n",
    "        df = df.dropna(subset=['days_to_death', 'days_to_followup'], how='all')\n",
    "        df = df.loc[df['case_id'].isin(disease_caseids[disease])]\n",
    "        \n",
    "        imp = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "        x = np.asarray([[float(y) if y != 'NA' else np.nan for y in x[1:-1].split(\", \")] for x in list(df['features'])])\n",
    "        x_mut = np.asarray([[float(y) if y != 'NA' else np.nan for y in x[1:-1].split(\", \")] for x in list(df['mut_features'])])\n",
    "        x_exp = np.asarray([[float(y) if y != 'NA' else np.nan for y in x[1:-1].split(\", \")] for x in list(df['exp_features'])])\n",
    "        mut_load = np.sum(x_mut, axis=1).reshape(df.shape[0], 1)\n",
    "        x = np.hstack((x, mut_load))\n",
    "        x = np.hstack((x, np.expand_dims(np.asarray(df['days_to_birth']), axis=1)))\n",
    "        x = np.hstack((x, x_exp))\n",
    "        x = np.where(np.isnan(x), ma.array(x, mask=np.isnan(x)).mean(axis=0), x)\n",
    "        imp.fit(x)\n",
    "        x = imp.transform(x)\n",
    "        scaler = preprocessing.StandardScaler().fit(x)\n",
    "        \n",
    "        disease_idx = disease_cols.index('disease_code_' + disease)\n",
    "        \n",
    "        test = df.loc[~df['case_id'].isin(runs_constrained_caseids[run])]\n",
    "        df = df.loc[df['case_id'].isin(runs_constrained_caseids[run])]\n",
    "        df = df.sample(frac=1)\n",
    "        train_size = int(0.75 * df.shape[0])\n",
    "        test_size = df.shape[0] - train_size\n",
    "        train = df.head(train_size)\n",
    "        validate = df.tail(test_size)\n",
    "        output_cols = ['days_to_death', 'days_to_followup']\n",
    "        dtb_train = np.expand_dims(np.asarray(train['days_to_birth']), axis=1)\n",
    "        mut_train = np.asarray([[float(y) if y != 'NA' else np.nan for y in x[1:-1].split(\", \")] for x in list(train['mut_features'])])\n",
    "        mut_train_load = np.sum(mut_train, axis=1).reshape(train.shape[0], 1)\n",
    "        mut_train = mut_train[:, best_mut_feats]\n",
    "        exp_train = np.asarray([[float(y) if y != 'NA' else np.nan for y in x[1:-1].split(\", \")] for x in list(train['exp_features'])])\n",
    "        x_train = np.asarray([[float(y) if y != 'NA' else np.nan for y in x[1:-1].split(\", \")] for x in list(train['features'])])\n",
    "        x_train = np.hstack((x_train, mut_train_load))\n",
    "        x_train = np.hstack((x_train, dtb_train))\n",
    "        x_train = np.hstack((x_train, exp_train))\n",
    "        x_train = imp.transform(x_train)\n",
    "        x_train = scaler.transform(x_train)\n",
    "        x_train = np.hstack((x_train, mut_train))\n",
    "        zero_mat = np.zeros((x_train.shape[0], len(disease_cols)))\n",
    "        zero_mat[:, disease_idx] = np.ones(x_train.shape[0])\n",
    "        x_train = np.hstack((x_train, zero_mat))\n",
    "        days = np.asarray(train[output_cols])\n",
    "        y_train = np.asarray([x[0] if np.isnan(x[0]) == False else x[1] for x in days])\n",
    "        e_train = np.asarray([1 if np.isnan(x[0]) == False else 0 for x in days])\n",
    "        y_train=nnet_survival.make_surv_array(y_train,[True if x == 1 else False for x in e_train],breaks)\n",
    "        \n",
    "        dtb_test = np.expand_dims(np.asarray(test['days_to_birth']), axis=1)\n",
    "        mut_test = np.asarray([[float(y) if y != 'NA' else np.nan for y in x[1:-1].split(\", \")] for x in list(test['mut_features'])])\n",
    "        mut_test_load = np.sum(mut_test, axis=1).reshape(test.shape[0], 1)\n",
    "        mut_test = mut_test[:, best_mut_feats]\n",
    "        exp_test = np.asarray([[float(y) if y != 'NA' else np.nan for y in x[1:-1].split(\", \")] for x in list(test['exp_features'])])\n",
    "        x_test = np.asarray([[float(y) if y != 'NA' else np.nan for y in x[1:-1].split(\", \")] for x in list(test['features'])])\n",
    "        x_test = np.hstack((x_test, mut_test_load))\n",
    "        x_test = np.hstack((x_test, dtb_test))\n",
    "        x_test = np.hstack((x_test, exp_test))\n",
    "        x_test = imp.transform(x_test)\n",
    "        x_test = scaler.transform(x_test)\n",
    "        x_test = np.hstack((x_test, mut_test))\n",
    "        zero_mat = np.zeros((x_test.shape[0], len(disease_cols)))\n",
    "        zero_mat[:, disease_idx] = np.ones(x_test.shape[0])\n",
    "        x_test = np.hstack((x_test, zero_mat))\n",
    "        days = np.asarray(test[output_cols])\n",
    "        y_test = np.asarray([x[0] if np.isnan(x[0]) == False else x[1] for x in days])\n",
    "        runs_disease_y_test_orig[(run, disease)] = y_test\n",
    "        e_test = np.asarray([1 if np.isnan(x[0]) == False else 0 for x in days])\n",
    "        y_test=nnet_survival.make_surv_array(y_test,[True if x == 1 else False for x in e_test],breaks)\n",
    "        \n",
    "        dtb_val = np.expand_dims(np.asarray(validate['days_to_birth']), axis=1)\n",
    "        mut_val = np.asarray([[float(y) if y != 'NA' else np.nan for y in x[1:-1].split(\", \")] for x in list(validate['mut_features'])])\n",
    "        mut_val_load = np.sum(mut_val, axis=1).reshape(validate.shape[0], 1)\n",
    "        mut_val = mut_val[:, best_mut_feats]\n",
    "        exp_val = np.asarray([[float(y) if y != 'NA' else np.nan for y in x[1:-1].split(\", \")] for x in list(validate['exp_features'])])\n",
    "        x_val = np.asarray([[float(y) if y != 'NA' else np.nan for y in x[1:-1].split(\", \")] for x in list(validate['features'])])\n",
    "        x_val = np.hstack((x_val, mut_val_load))\n",
    "        x_val = np.hstack((x_val, dtb_val))\n",
    "        x_val = np.hstack((x_val, exp_val))\n",
    "        x_val = imp.transform(x_val)\n",
    "        x_val = scaler.transform(x_val)\n",
    "        x_val = np.hstack((x_val, mut_val))\n",
    "        zero_mat = np.zeros((x_val.shape[0], len(disease_cols)))\n",
    "        zero_mat[:, disease_idx] = np.ones(x_val.shape[0])\n",
    "        x_val = np.hstack((x_val, zero_mat))\n",
    "        days = np.asarray(validate[output_cols])\n",
    "        y_val = np.asarray([x[0] if np.isnan(x[0]) == False else x[1] for x in days])\n",
    "        e_val = np.asarray([1 if np.isnan(x[0]) == False else 0 for x in days])\n",
    "        y_val=nnet_survival.make_surv_array(y_val,[True if x == 1 else False for x in e_val],breaks)\n",
    "        \n",
    "        print(x_train.shape, x_val.shape, x_test.shape)\n",
    "        \n",
    "        if x_train.shape[1] != x_val.shape[1] or x_train.shape[1] != x_test.shape[1] or x_val.shape[1] != x_test.shape[1]:\n",
    "            print(\"mismatched shape\")\n",
    "            continue\n",
    "\n",
    "        if np.sum(e_train) < 0.3 * len(e_train) or np.sum(e_val) < 0.3 * len(e_val) or np.sum(e_test) < 0.3 * len(e_test) or len(e_train) < 150:\n",
    "            print(np.sum(e_train), len(e_train), np.sum(e_val), len(e_val), np.sum(e_test), len(e_test))\n",
    "            print(\"skipping \", disease)\n",
    "            continue\n",
    "\n",
    "        runs_disease_x_train[(run, disease)] = x_train\n",
    "        runs_disease_x_test[(run, disease)] = x_test\n",
    "        runs_disease_x_val[(run, disease)] = x_val\n",
    "        runs_disease_y_train[(run, disease)] = y_train\n",
    "        runs_disease_y_test[(run, disease)] = y_test\n",
    "        runs_disease_y_val[(run, disease)] = y_val\n",
    "        runs_disease_e_train[(run, disease)] = e_train\n",
    "        runs_disease_e_test[(run, disease)] = e_test\n",
    "        runs_disease_e_val[(run, disease)] = e_val\n",
    "        runs_disease_n_intervals[(run, disease)] = n_intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tuning for baseline models trained on global model weights\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Masking\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Flatten\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras import optimizers\n",
    "from keras import regularizers\n",
    "import operator\n",
    "\n",
    "runs_disease_trained_global_errors = {}\n",
    "learning_rates = [0.1, 0.01, 0.001, 0.0001, 0.00001]\n",
    "for run in range(10):\n",
    "    for disease in [item[1] for item in runs_disease_x_train.keys() if item[0] == run]:\n",
    "        for learning_rate in learning_rates:\n",
    "            model = keras.models.clone_model(runs_global_models[run])\n",
    "            model.set_weights(runs_global_models[run].get_weights())\n",
    "            adam = optimizers.Adam(lr=learning_rate)\n",
    "            model.compile(loss=nnet_survival.surv_likelihood(runs_disease_n_intervals[(run, disease)]), optimizer=adam)\n",
    "            early_stopping = EarlyStopping(monitor='val_loss', patience=2)\n",
    "            history = model.fit(runs_disease_x_train[(run, disease)], runs_disease_y_train[(run, disease)], batch_size=256, epochs=1000, shuffle=True,\n",
    "                               callbacks=[early_stopping], validation_data=(runs_disease_x_val[(run, disease)], runs_disease_y_val[(run, disease)]))\n",
    "            best_epochs = history.history['val_loss'].index(min(history.history['val_loss'])) + 1\n",
    "            runs_disease_trained_global_errors[(run, (disease, learning_rate, best_epochs))] = min(history.history['val_loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train each transfer model using global model weights for initialization\n",
    "\n",
    "runs_disease_trained_global_models = {}\n",
    "for run in range(10):\n",
    "    for disease in [item[1] for item in runs_disease_x_train.keys() if item[0] == run]:\n",
    "        this_disease_runs = [x for x in runs_disease_trained_global_errors.keys() if disease == x[1][0] and run == x[0]]\n",
    "        lowest_error = 100000\n",
    "        best_run = None\n",
    "        for disease_run in this_disease_runs:\n",
    "            if runs_disease_trained_global_errors[disease_run] < lowest_error:\n",
    "                lowest_error = runs_disease_trained_global_errors[disease_run]\n",
    "                best_run = disease_run\n",
    "        print(best_run, runs_disease_trained_global_errors[best_run])\n",
    "        total_x = np.vstack((runs_disease_x_train[(run, disease)], runs_disease_x_val[(run, disease)]))\n",
    "        total_y = np.vstack((runs_disease_y_train[(run, disease)], runs_disease_y_val[(run, disease)]))\n",
    "        total_e = np.hstack((runs_disease_e_train[(run, disease)], runs_disease_e_val[(run, disease)]))\n",
    "        model = keras.models.clone_model(runs_global_models[run])\n",
    "        model.set_weights(runs_global_models[run].get_weights())\n",
    "        adam = optimizers.Adam(lr=best_run[1][1])\n",
    "        model.compile(loss=nnet_survival.surv_likelihood(runs_disease_n_intervals[(run, disease)]), optimizer=adam)\n",
    "        model.fit(total_x, total_y, batch_size=256, epochs=best_run[1][2], shuffle=True)\n",
    "        runs_disease_trained_global_models[(run, disease)] = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Test each transfer model using global model weights for initialization\n",
    "\n",
    "runs_disease_trained_global_loss = {}\n",
    "for run in range(10):\n",
    "    for disease in [item[1] for item in runs_disease_x_train.keys() if item[0] == run]:\n",
    "        test_loss = runs_disease_trained_global_models[(run, disease)].evaluate(runs_disease_x_test[(run, disease)], runs_disease_y_test[(run, disease)])\n",
    "        runs_disease_trained_global_loss[(run, disease)] = test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tuning for baseline models\n",
    "# Find best hyperparameters for baseline model for each disease\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Masking\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Flatten\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras import optimizers\n",
    "from keras import regularizers\n",
    "import nnet_survival\n",
    "\n",
    "patience = 2\n",
    "runs_disease_baseline_errors = {}\n",
    "units = [10, 50, 100, 200, 500, 1000]\n",
    "learning_rates = [0.1, 0.01, 0.001, 0.0001, 0.00001]\n",
    "for run in range(10):\n",
    "    for disease in [item[1] for item in runs_disease_x_train.keys() if item[0] == run]:\n",
    "        for unit in units:\n",
    "            for learning_rate in learning_rates:\n",
    "                print(run, disease, unit, learning_rate)\n",
    "                num_epochs = 0\n",
    "                min_loss = 1000000\n",
    "                num_patience = 0\n",
    "                model = Sequential()\n",
    "                model.add(Dense(unit, activation='relu', input_shape=(runs_disease_x_train[(run, disease)].shape[1],)))\n",
    "                model.add(Dense(runs_disease_n_intervals[(run, disease)], activation='sigmoid'))\n",
    "                adam = optimizers.Adam(lr=learning_rate)\n",
    "                model.compile(loss=nnet_survival.surv_likelihood(runs_disease_n_intervals[(run, disease)]), optimizer=adam)\n",
    "                while True:\n",
    "                    model.fit(runs_disease_x_train[(run, disease)], runs_disease_y_train[(run, disease)], batch_size=256, epochs=1, shuffle=True, verbose=0)\n",
    "                    loss = model.evaluate(runs_disease_x_val[(run, disease)], runs_disease_y_val[(run, disease)], verbose=0)\n",
    "                    if loss < min_loss:\n",
    "                        num_epochs += 1\n",
    "                        min_loss = loss\n",
    "                        num_patience = 0\n",
    "                    else:\n",
    "                        if num_patience < patience:\n",
    "                            num_epochs += 1\n",
    "                            num_patience += 1\n",
    "                        else:\n",
    "                            break\n",
    "                runs_disease_baseline_errors[(run, (disease, learning_rate, num_epochs - patience, unit))] = min_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train each baseline model\n",
    "\n",
    "runs_disease_baseline_models = {}\n",
    "for run in range(10):\n",
    "    for disease in [item[1] for item in runs_disease_x_train.keys() if item[0] == run]:\n",
    "        this_disease_runs = [x for x in runs_disease_baseline_errors.keys() if disease == x[1][0] and run == x[0]]\n",
    "        lowest_error = 100000\n",
    "        best_run = None\n",
    "        for disease_run in this_disease_runs:\n",
    "            if runs_disease_baseline_errors[disease_run] < lowest_error:\n",
    "                lowest_error = runs_disease_baseline_errors[disease_run]\n",
    "                best_run = disease_run\n",
    "        print(best_run, runs_disease_baseline_errors[best_run])\n",
    "        total_x = np.vstack((runs_disease_x_train[(run, disease)], runs_disease_x_val[(run, disease)]))\n",
    "        total_y = np.vstack((runs_disease_y_train[(run, disease)], runs_disease_y_val[(run, disease)]))\n",
    "        total_e = np.hstack((runs_disease_e_train[(run, disease)], runs_disease_e_val[(run, disease)]))\n",
    "        model = Sequential()\n",
    "        model.add(Dense(best_run[1][3], activation='relu', input_shape=(total_x.shape[1],)))\n",
    "        model.add(Dense(runs_disease_n_intervals[(run, disease)], activation='sigmoid'))\n",
    "        adam = optimizers.Adam(lr=best_run[1][1])\n",
    "        model.compile(loss=nnet_survival.surv_likelihood(runs_disease_n_intervals[(run, disease)]), optimizer=adam)\n",
    "        model.fit(total_x, total_y, batch_size=256, epochs=best_run[1][2], shuffle=True)\n",
    "        runs_disease_baseline_models[(run, disease)] = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Test each baseline model\n",
    "\n",
    "runs_disease_baseline_loss = {}\n",
    "for run in range(10):\n",
    "    for disease in [item[1] for item in runs_disease_x_train.keys() if item[0] == run]:\n",
    "        test_loss = runs_disease_baseline_models[(run, disease)].evaluate(runs_disease_x_test[(run, disease)], runs_disease_y_test[(run, disease)])\n",
    "        runs_disease_baseline_loss[(run, disease)] = test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe id=\"igraph\" scrolling=\"no\" style=\"border:none;\" seamless=\"seamless\" src=\"https://plot.ly/~nlin5/349.embed\" height=\"525px\" width=\"100%\"></iframe>"
      ],
      "text/plain": [
       "<plotly.tools.PlotlyDisplay object>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "disease_diff = {}\n",
    "for key in runs_disease_trained_global_loss.keys():\n",
    "    diff = runs_disease_baseline_loss[key] - runs_disease_trained_global_loss[key]\n",
    "    if key[1] not in disease_diff.keys():\n",
    "        disease_diff[key[1]] = []\n",
    "        disease_diff[key[1]].append(diff)\n",
    "    else:\n",
    "        disease_diff[key[1]].append(diff)\n",
    "        \n",
    "import plotly.plotly as py\n",
    "import plotly.graph_objs as go\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "data = [go.Box(y = disease_diff[x], name=x) for x in disease_diff.keys()]\n",
    "py.iplot(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
