{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set backend for Keras to be Theano\n",
    "# You can make multiple Keras models this way w/o slowing down, unlike Tensorflow\n",
    "\n",
    "from keras import backend as K\n",
    "import os\n",
    "from importlib import reload\n",
    "\n",
    "def set_keras_backend(backend):\n",
    "\n",
    "    if K.backend() != backend:\n",
    "        os.environ['KERAS_BACKEND'] = backend\n",
    "        reload(K)\n",
    "        assert K.backend() == backend\n",
    "\n",
    "set_keras_backend(\"theano\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Edit this to change your directory to TransferSRNN directory\n",
    "\n",
    "%cd /home/nolelin/TransferSRNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.models import Sequential, Model\n",
    "from keras import optimizers, layers, regularizers\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.layers import Input, Dense, Activation\n",
    "from keras.models import load_model\n",
    "from lifelines import KaplanMeierFitter\n",
    "from lifelines import CoxPHFitter\n",
    "from lifelines.utils import concordance_index\n",
    "from nnet_survival import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation, Dropout\n",
    "from keras.optimizers import SGD, RMSprop\n",
    "import theano.tensor as T\n",
    "\n",
    "from lifelines.utils import concordance_index\n",
    "from lifelines import CoxPHFitter\n",
    "\n",
    "from lifelines.datasets import load_rossi\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Get disease codes for each disease type that we want to make comparisons between baseline/transfer\n",
    "\n",
    "df = pd.read_csv('binding/all_features/all_features.csv')\n",
    "disease_names = list(df.disease_type.unique())\n",
    "disease_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all case_ids of patients for each disease type\n",
    "\n",
    "disease_caseids = {}\n",
    "df2 = pd.read_csv('binding/all_features/all_features.csv')\n",
    "for disease in disease_names:\n",
    "    diseasedf = df2.loc[df2['disease_type'] == disease]\n",
    "    disease_caseids[disease] = list(diseasedf['case_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only train/validate on 80% of all patients\n",
    "# Do 10 runs\n",
    "\n",
    "from random import shuffle\n",
    "\n",
    "runs_constrained_caseids = {}\n",
    "for run in range(10):\n",
    "    constrained_caseids = []\n",
    "    for disease in disease_caseids.keys():\n",
    "        num_patients = len(disease_caseids[disease])\n",
    "        shuffle(disease_caseids[disease])\n",
    "        constrained_caseids.extend(disease_caseids[disease][0:int(0.8 * num_patients)])\n",
    "    runs_constrained_caseids[run] = constrained_caseids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean and preprocess data for global model\n",
    "\n",
    "import numpy as np\n",
    "import sys\n",
    "import numpy.ma as ma\n",
    "from sklearn import preprocessing\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "runs_x_train = {}\n",
    "runs_x_test = {}\n",
    "runs_x_val = {}\n",
    "runs_y_train = {}\n",
    "runs_y_test = {}\n",
    "runs_y_val = {}\n",
    "runs_e_train = {}\n",
    "runs_e_test = {}\n",
    "runs_e_val = {}\n",
    "runs_n_intervals = {}\n",
    "runs_breaks = {}\n",
    "    \n",
    "for run in range(10):\n",
    "    print(run)\n",
    "    disease_dfs = []\n",
    "    for disease in disease_names:\n",
    "        df = pd.read_csv('binding/all_features/all_features.csv')\n",
    "        df = df.dropna(subset=['days_to_death', 'days_to_followup'], how='all')\n",
    "        df = df.loc[df['case_id'].isin(disease_caseids[disease])]\n",
    "        df['disease_code'] = disease\n",
    "        disease_dfs.append(df)\n",
    "    df = pd.concat(disease_dfs)\n",
    "    df = pd.get_dummies(df, columns=['disease_code'])\n",
    "    disease_cols = [x for x in df.columns if 'disease_code' in x]\n",
    "    \n",
    "    output_cols = ['days_to_death', 'days_to_followup']\n",
    "    imp = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "    x_mut = np.asarray([[float(y) if y != 'NA' else np.nan for y in x[1:-1].split(\", \")] for x in list(df['mut_features'])])\n",
    "    x_exp = np.asarray([[float(y) if y != 'NA' else np.nan for y in x[1:-1].split(\", \")] for x in list(df['exp_features'])])\n",
    "    mut_load = np.sum(x_mut, axis=1).reshape(df.shape[0], 1)\n",
    "    x = np.hstack((x_exp, mut_load))\n",
    "    x = np.hstack((x, np.expand_dims(np.asarray(df['days_to_birth']), axis=1)))\n",
    "    x = np.where(np.isnan(x), ma.array(x, mask=np.isnan(x)).mean(axis=0), x)\n",
    "    imp.fit(x)\n",
    "    x = imp.transform(x)\n",
    "    scaler = preprocessing.StandardScaler().fit(x)\n",
    "    days = np.asarray(df[output_cols])\n",
    "    y_whole = np.asarray([x[0] if np.isnan(x[0]) == False else x[1] for x in days])\n",
    "    breaks=np.arange(min(y_whole), max(y_whole), 200)\n",
    "    n_intervals=len(breaks)-1\n",
    "    \n",
    "    test = df.loc[~df['case_id'].isin(runs_constrained_caseids[run])]\n",
    "    df = df.loc[df['case_id'].isin(runs_constrained_caseids[run])]\n",
    "    df = df.sample(frac=1)\n",
    "    train_size = int(0.75 * df.shape[0])\n",
    "    test_size = df.shape[0] - train_size\n",
    "    train = df.head(train_size)\n",
    "    validate = df.tail(test_size)\n",
    "    dtb_train = np.expand_dims(np.asarray(train['days_to_birth']), axis=1)\n",
    "    mut_train = np.asarray([[float(y) if y != 'NA' else np.nan for y in x[1:-1].split(\", \")] for x in list(train['mut_features'])])\n",
    "    mut_train_load = np.sum(mut_train, axis=1).reshape(train.shape[0], 1)\n",
    "    exp_train = np.asarray([[float(y) if y != 'NA' else np.nan for y in x[1:-1].split(\", \")] for x in list(train['exp_features'])])\n",
    "    x_train = np.hstack((exp_train, mut_train_load))\n",
    "    x_train = np.hstack((x_train, dtb_train))\n",
    "    x_train = imp.transform(x_train)\n",
    "    x_train = scaler.transform(x_train)\n",
    "    x_train = x_train[:, ~np.all(x_train == 0, axis=0)]\n",
    "    x_train = np.hstack((x_train, mut_train))\n",
    "    x_train = np.hstack((x_train, train[disease_cols]))\n",
    "    days = np.asarray(train[output_cols])\n",
    "    y_train = np.asarray([x[0] if np.isnan(x[0]) == False else x[1] for x in days])\n",
    "    e_train = np.asarray([1 if np.isnan(x[0]) == False else 0 for x in days])\n",
    "    y_train=make_surv_array(y_train,[True if x == 1 else False for x in e_train],breaks)\n",
    "        \n",
    "    dtb_test = np.expand_dims(np.asarray(test['days_to_birth']), axis=1)\n",
    "    mut_test = np.asarray([[float(y) if y != 'NA' else np.nan for y in x[1:-1].split(\", \")] for x in list(test['mut_features'])])\n",
    "    mut_test_load = np.sum(mut_test, axis=1).reshape(test.shape[0], 1)\n",
    "    exp_test = np.asarray([[float(y) if y != 'NA' else np.nan for y in x[1:-1].split(\", \")] for x in list(test['exp_features'])])\n",
    "    x_test = np.hstack((exp_test, mut_test_load))\n",
    "    x_test = np.hstack((x_test, dtb_test))\n",
    "    x_test = imp.transform(x_test)\n",
    "    x_test = scaler.transform(x_test)\n",
    "    x_test = x_test[:, ~np.all(x_test == 0, axis=0)]\n",
    "    x_test = np.hstack((x_test, mut_test))\n",
    "    x_test = np.hstack((x_test, test[disease_cols]))\n",
    "    days = np.asarray(test[output_cols])\n",
    "    y_test = np.asarray([x[0] if np.isnan(x[0]) == False else x[1] for x in days])\n",
    "    e_test = np.asarray([1 if np.isnan(x[0]) == False else 0 for x in days])\n",
    "    y_test=make_surv_array(y_test,[True if x == 1 else False for x in e_test],breaks)\n",
    "    \n",
    "    dtb_val = np.expand_dims(np.asarray(validate['days_to_birth']), axis=1)\n",
    "    mut_val = np.asarray([[float(y) if y != 'NA' else np.nan for y in x[1:-1].split(\", \")] for x in list(validate['mut_features'])])\n",
    "    mut_val_load = np.sum(mut_val, axis=1).reshape(validate.shape[0], 1)\n",
    "    exp_val = np.asarray([[float(y) if y != 'NA' else np.nan for y in x[1:-1].split(\", \")] for x in list(validate['exp_features'])])\n",
    "    x_val = np.hstack((exp_val, mut_val_load))\n",
    "    x_val = np.hstack((x_val, dtb_val))\n",
    "    x_val = imp.transform(x_val)\n",
    "    x_val = scaler.transform(x_val)\n",
    "    x_val = x_val[:, ~np.all(x_val == 0, axis=0)]\n",
    "    x_val = np.hstack((x_val, mut_val))\n",
    "    x_val = np.hstack((x_val, validate[disease_cols]))\n",
    "    days = np.asarray(validate[output_cols])\n",
    "    y_val = np.asarray([x[0] if np.isnan(x[0]) == False else x[1] for x in days])\n",
    "    e_val = np.asarray([1 if np.isnan(x[0]) == False else 0 for x in days])\n",
    "    y_val=make_surv_array(y_val,[True if x == 1 else False for x in e_val],breaks)\n",
    "    \n",
    "    print(x_train.shape, x_val.shape, x_test.shape)\n",
    "        \n",
    "    if x_train.shape[1] != x_val.shape[1] or x_train.shape[1] != x_test.shape[1] or x_val.shape[1] != x_test.shape[1]:\n",
    "        print(\"mismatched shape\")\n",
    "        continue\n",
    "\n",
    "    if np.sum(e_train) < 0.2 * len(e_train) or np.sum(e_val) < 0.2 * len(e_val) or np.sum(e_test) < 0.2 * len(e_test) or len(e_train) < 150:\n",
    "        print(\"skipping \", disease)\n",
    "        continue\n",
    "    \n",
    "    runs_x_train[run] = x_train\n",
    "    runs_x_test[run] = x_test\n",
    "    runs_x_val[run] = x_val\n",
    "    runs_y_train[run] = y_train\n",
    "    runs_y_test[run] = y_test\n",
    "    runs_y_val[run] = y_val\n",
    "    runs_e_train[run] = e_train\n",
    "    runs_e_test[run] = e_test\n",
    "    runs_e_val[run] = e_val\n",
    "    runs_n_intervals[run] = n_intervals\n",
    "    runs_breaks[run] = breaks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Tuning for global models\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Masking\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Flatten\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras import optimizers\n",
    "from keras import regularizers\n",
    "import sys\n",
    "\n",
    "runs_global_errors = {}\n",
    "units = [10, 50, 100, 200, 500, 1000]\n",
    "learning_rates = [0.1, 0.01, 0.001, 0.0001, 0.00001]\n",
    "for run in range(10):\n",
    "    for unit in units:\n",
    "        for learning_rate in learning_rates:\n",
    "            model = Sequential()\n",
    "            model.add(Dense(unit, activation='relu', input_shape=(runs_x_train[run].shape[1],)))\n",
    "            model.add(BatchNormalization())\n",
    "            model.add(Dense(unit, activation='relu'))\n",
    "            model.add(BatchNormalization())\n",
    "            model.add(Dense(runs_n_intervals[run], activation='sigmoid'))\n",
    "            adam = optimizers.Adam(lr=learning_rate)\n",
    "            model.compile(loss=surv_likelihood(runs_n_intervals[run]), optimizer=adam)\n",
    "            early_stopping = EarlyStopping(monitor='val_loss', patience=2)\n",
    "            history = model.fit(runs_x_train[run], runs_y_train[run], batch_size=256, epochs=1000, shuffle=True, callbacks=[early_stopping], validation_data=(runs_x_val[run], runs_y_val[run]))\n",
    "            best_epochs = history.history['val_loss'].index(min(history.history['val_loss'])) + 1\n",
    "            runs_global_errors[(run, (learning_rate, best_epochs, unit))] = min(history.history['val_loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Train global models\n",
    "\n",
    "runs_global_models = {}\n",
    "for run in range(10):\n",
    "    this_runs = [x for x in runs_global_errors.keys() if run == x[0]]\n",
    "    lowest_loss = 10000\n",
    "    best_run = None\n",
    "    for this_run in this_runs:\n",
    "        if runs_global_errors[this_run] < lowest_loss:\n",
    "            lowest_loss = runs_global_errors[this_run]\n",
    "            best_run = this_run\n",
    "    print(best_run, runs_global_errors[best_run])\n",
    "    total_x = np.vstack((runs_x_train[run], runs_x_val[run]))\n",
    "    total_y = np.vstack((runs_y_train[run], runs_y_val[run]))\n",
    "    total_e = np.hstack((runs_e_train[run], runs_e_val[run]))\n",
    "    model = Sequential()\n",
    "    model.add(Dense(best_run[1][2], activation='relu', input_shape=(total_x.shape[1],)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dense(best_run[1][2], activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dense(runs_n_intervals[run], activation='sigmoid'))\n",
    "    adam = optimizers.Adam(lr=best_run[1][0])\n",
    "    model.compile(loss=surv_likelihood(runs_n_intervals[run]), optimizer=adam)\n",
    "    model.fit(total_x, total_y, batch_size=256, epochs=best_run[1][1], shuffle=True)\n",
    "    runs_global_models[run] = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do same data pre-processing for each individual disease\n",
    "# Don't include diseases that don't have at least 150 patients in training set and at least 30% of patients have died\n",
    "\n",
    "import numpy as np\n",
    "import sys\n",
    "import numpy.ma as ma\n",
    "from sklearn import preprocessing\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "runs_disease_x_train = {}\n",
    "runs_disease_x_test = {}\n",
    "runs_disease_x_val = {}\n",
    "runs_disease_y_train = {}\n",
    "runs_disease_y_test = {}\n",
    "runs_disease_y_val = {}\n",
    "runs_disease_y_test_orig = {}\n",
    "runs_disease_e_train = {}\n",
    "runs_disease_e_test = {}\n",
    "runs_disease_e_val = {}\n",
    "runs_disease_n_intervals = {}\n",
    "\n",
    "for run in range(10):\n",
    "    breaks = runs_breaks[run]\n",
    "    n_intervals=len(breaks)-1\n",
    "    for disease in disease_names:\n",
    "        df = pd.read_csv('binding/all_features/all_features.csv')\n",
    "        df = df.dropna(subset=['days_to_death', 'days_to_followup'], how='all')\n",
    "        df = df.loc[df['case_id'].isin(disease_caseids[disease])]\n",
    "        \n",
    "        imp = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "        x_mut = np.asarray([[float(y) if y != 'NA' else np.nan for y in x[1:-1].split(\", \")] for x in list(df['mut_features'])])\n",
    "        x_exp = np.asarray([[float(y) if y != 'NA' else np.nan for y in x[1:-1].split(\", \")] for x in list(df['exp_features'])])\n",
    "        mut_load = np.sum(x_mut, axis=1).reshape(df.shape[0], 1)\n",
    "        x = np.hstack((x_exp, mut_load))\n",
    "        x = np.hstack((x, np.expand_dims(np.asarray(df['days_to_birth']), axis=1)))\n",
    "        x = np.where(np.isnan(x), ma.array(x, mask=np.isnan(x)).mean(axis=0), x)\n",
    "        imp.fit(x)\n",
    "        x = imp.transform(x)\n",
    "        scaler = preprocessing.StandardScaler().fit(x)\n",
    "        \n",
    "        disease_idx = disease_cols.index('disease_code_' + disease)\n",
    "        \n",
    "        test = df.loc[~df['case_id'].isin(runs_constrained_caseids[run])]\n",
    "        df = df.loc[df['case_id'].isin(runs_constrained_caseids[run])]\n",
    "        df = df.sample(frac=1)\n",
    "        train_size = int(0.75 * df.shape[0])\n",
    "        test_size = df.shape[0] - train_size\n",
    "        train = df.head(train_size)\n",
    "        validate = df.tail(test_size)\n",
    "        output_cols = ['days_to_death', 'days_to_followup']\n",
    "        dtb_train = np.expand_dims(np.asarray(train['days_to_birth']), axis=1)\n",
    "        mut_train = np.asarray([[float(y) if y != 'NA' else np.nan for y in x[1:-1].split(\", \")] for x in list(train['mut_features'])])\n",
    "        mut_train_load = np.sum(mut_train, axis=1).reshape(train.shape[0], 1)\n",
    "        exp_train = np.asarray([[float(y) if y != 'NA' else np.nan for y in x[1:-1].split(\", \")] for x in list(train['exp_features'])])\n",
    "        x_train = np.hstack((exp_train, mut_train_load))\n",
    "        x_train = np.hstack((x_train, dtb_train))\n",
    "        x_train = imp.transform(x_train)\n",
    "        x_train = scaler.transform(x_train)\n",
    "        x_train = x_train[:, ~np.all(x_train == 0, axis=0)]\n",
    "        x_train = np.hstack((x_train, mut_train))\n",
    "        zero_mat = np.zeros((x_train.shape[0], len(disease_cols)))\n",
    "        zero_mat[:, disease_idx] = np.ones(x_train.shape[0])\n",
    "        x_train = np.hstack((x_train, zero_mat))\n",
    "        days = np.asarray(train[output_cols])\n",
    "        y_train = np.asarray([x[0] if np.isnan(x[0]) == False else x[1] for x in days])\n",
    "        e_train = np.asarray([1 if np.isnan(x[0]) == False else 0 for x in days])\n",
    "        y_train=make_surv_array(y_train,[True if x == 1 else False for x in e_train],breaks)\n",
    "        \n",
    "        dtb_test = np.expand_dims(np.asarray(test['days_to_birth']), axis=1)\n",
    "        mut_test = np.asarray([[float(y) if y != 'NA' else np.nan for y in x[1:-1].split(\", \")] for x in list(test['mut_features'])])\n",
    "        mut_test_load = np.sum(mut_test, axis=1).reshape(test.shape[0], 1)\n",
    "        exp_test = np.asarray([[float(y) if y != 'NA' else np.nan for y in x[1:-1].split(\", \")] for x in list(test['exp_features'])])\n",
    "        x_test = np.hstack((exp_test, mut_test_load))\n",
    "        x_test = np.hstack((x_test, dtb_test))\n",
    "        x_test = imp.transform(x_test)\n",
    "        x_test = scaler.transform(x_test)\n",
    "        x_test = x_test[:, ~np.all(x_test == 0, axis=0)]\n",
    "        x_test = np.hstack((x_test, mut_test))\n",
    "        zero_mat = np.zeros((x_test.shape[0], len(disease_cols)))\n",
    "        zero_mat[:, disease_idx] = np.ones(x_test.shape[0])\n",
    "        x_test = np.hstack((x_test, zero_mat))\n",
    "        days = np.asarray(test[output_cols])\n",
    "        y_test = np.asarray([x[0] if np.isnan(x[0]) == False else x[1] for x in days])\n",
    "        runs_disease_y_test_orig[(run, disease)] = y_test\n",
    "        e_test = np.asarray([1 if np.isnan(x[0]) == False else 0 for x in days])\n",
    "        y_test=make_surv_array(y_test,[True if x == 1 else False for x in e_test],breaks)\n",
    "        \n",
    "        dtb_val = np.expand_dims(np.asarray(validate['days_to_birth']), axis=1)\n",
    "        mut_val = np.asarray([[float(y) if y != 'NA' else np.nan for y in x[1:-1].split(\", \")] for x in list(validate['mut_features'])])\n",
    "        mut_val_load = np.sum(mut_val, axis=1).reshape(validate.shape[0], 1)\n",
    "        exp_val = np.asarray([[float(y) if y != 'NA' else np.nan for y in x[1:-1].split(\", \")] for x in list(validate['exp_features'])])\n",
    "        x_val = np.hstack((exp_val, mut_val_load))\n",
    "        x_val = np.hstack((x_val, dtb_val))\n",
    "        x_val = imp.transform(x_val)\n",
    "        x_val = scaler.transform(x_val)\n",
    "        x_val = x_val[:, ~np.all(x_val == 0, axis=0)]\n",
    "        x_val = np.hstack((x_val, mut_val))\n",
    "        zero_mat = np.zeros((x_val.shape[0], len(disease_cols)))\n",
    "        zero_mat[:, disease_idx] = np.ones(x_val.shape[0])\n",
    "        x_val = np.hstack((x_val, zero_mat))\n",
    "        days = np.asarray(validate[output_cols])\n",
    "        y_val = np.asarray([x[0] if np.isnan(x[0]) == False else x[1] for x in days])\n",
    "        e_val = np.asarray([1 if np.isnan(x[0]) == False else 0 for x in days])\n",
    "        y_val=make_surv_array(y_val,[True if x == 1 else False for x in e_val],breaks)\n",
    "        \n",
    "        print(x_train.shape, x_val.shape, x_test.shape)\n",
    "        \n",
    "        if x_train.shape[1] != x_val.shape[1] or x_train.shape[1] != x_test.shape[1] or x_val.shape[1] != x_test.shape[1]:\n",
    "            print(\"mismatched shape\")\n",
    "            continue\n",
    "\n",
    "        if np.sum(e_train) < 0.0 * len(e_train) or np.sum(e_val) < 0.0 * len(e_val) or np.sum(e_test) < 0.0 * len(e_test) or len(e_train) < 150:\n",
    "            print(np.sum(e_train), len(e_train), np.sum(e_val), len(e_val), np.sum(e_test), len(e_test))\n",
    "            print(\"skipping \", disease)\n",
    "            continue\n",
    "\n",
    "        runs_disease_x_train[(run, disease)] = x_train\n",
    "        runs_disease_x_test[(run, disease)] = x_test\n",
    "        runs_disease_x_val[(run, disease)] = x_val\n",
    "        runs_disease_y_train[(run, disease)] = y_train\n",
    "        runs_disease_y_test[(run, disease)] = y_test\n",
    "        runs_disease_y_val[(run, disease)] = y_val\n",
    "        runs_disease_e_train[(run, disease)] = e_train\n",
    "        runs_disease_e_test[(run, disease)] = e_test\n",
    "        runs_disease_e_val[(run, disease)] = e_val\n",
    "        runs_disease_n_intervals[(run, disease)] = n_intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Tuning for baseline models trained on global model weights\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Masking\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Flatten\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras import optimizers\n",
    "from keras import regularizers\n",
    "import operator\n",
    "\n",
    "runs_disease_trained_global_errors = {}\n",
    "learning_rates = [0.1, 0.01, 0.001, 0.0001, 0.00001]\n",
    "for run in range(10):\n",
    "    for disease in [item[1] for item in runs_disease_x_train.keys() if item[0] == run]:\n",
    "        for learning_rate in learning_rates:\n",
    "            model = keras.models.clone_model(runs_global_models[run])\n",
    "            model.set_weights(runs_global_models[run].get_weights())\n",
    "            adam = optimizers.Adam(lr=learning_rate)\n",
    "            model.compile(loss=surv_likelihood(runs_disease_n_intervals[(run, disease)]), optimizer=adam)\n",
    "            early_stopping = EarlyStopping(monitor='val_loss', patience=2)\n",
    "            history = model.fit(runs_disease_x_train[(run, disease)], runs_disease_y_train[(run, disease)], batch_size=256, epochs=1000, shuffle=True,\n",
    "                               callbacks=[early_stopping], validation_data=(runs_disease_x_val[(run, disease)], runs_disease_y_val[(run, disease)]))\n",
    "            best_epochs = history.history['val_loss'].index(min(history.history['val_loss'])) + 1\n",
    "            runs_disease_trained_global_errors[(run, (disease, learning_rate, best_epochs))] = min(history.history['val_loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Train each transfer model using global model weights for initialization\n",
    "\n",
    "runs_disease_trained_global_models = {}\n",
    "for run in range(10):\n",
    "    for disease in [item[1] for item in runs_disease_x_train.keys() if item[0] == run]:\n",
    "        this_disease_runs = [x for x in runs_disease_trained_global_errors.keys() if disease == x[1][0] and run == x[0]]\n",
    "        lowest_error = 100000\n",
    "        best_run = None\n",
    "        for disease_run in this_disease_runs:\n",
    "            if runs_disease_trained_global_errors[disease_run] < lowest_error:\n",
    "                lowest_error = runs_disease_trained_global_errors[disease_run]\n",
    "                best_run = disease_run\n",
    "        print(best_run, runs_disease_trained_global_errors[best_run])\n",
    "        total_x = np.vstack((runs_disease_x_train[(run, disease)], runs_disease_x_val[(run, disease)]))\n",
    "        total_y = np.vstack((runs_disease_y_train[(run, disease)], runs_disease_y_val[(run, disease)]))\n",
    "        total_e = np.hstack((runs_disease_e_train[(run, disease)], runs_disease_e_val[(run, disease)]))\n",
    "        model = keras.models.clone_model(runs_global_models[run])\n",
    "        model.set_weights(runs_global_models[run].get_weights())\n",
    "        adam = optimizers.Adam(lr=best_run[1][1])\n",
    "        model.compile(loss=surv_likelihood(runs_disease_n_intervals[(run, disease)]), optimizer=adam)\n",
    "        model.fit(total_x, total_y, batch_size=256, epochs=best_run[1][2], shuffle=True)\n",
    "        runs_disease_trained_global_models[(run, disease)] = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Test each transfer model using global model weights for initialization\n",
    "\n",
    "runs_disease_trained_global_loss = {}\n",
    "for run in range(10):\n",
    "    for disease in [item[1] for item in runs_disease_x_train.keys() if item[0] == run]:\n",
    "        test_loss = runs_disease_trained_global_models[(run, disease)].evaluate(runs_disease_x_test[(run, disease)], runs_disease_y_test[(run, disease)])\n",
    "        runs_disease_trained_global_loss[(run, disease)] = test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tuning for baseline models\n",
    "# Find best hyperparameters for baseline model for each disease\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Masking\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Flatten\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras import optimizers\n",
    "from keras import regularizers\n",
    "\n",
    "patience = 2\n",
    "runs_disease_baseline_errors = {}\n",
    "units = [10, 50, 100, 200, 500, 1000]\n",
    "learning_rates = [0.1, 0.01, 0.001, 0.0001, 0.00001]\n",
    "for run in range(10):\n",
    "    for disease in [item[1] for item in runs_disease_x_train.keys() if item[0] == run]:\n",
    "        for unit in units:\n",
    "            for learning_rate in learning_rates:\n",
    "                print(run, disease, unit, learning_rate)\n",
    "                num_epochs = 0\n",
    "                min_loss = 1000000\n",
    "                num_patience = 0\n",
    "                model = Sequential()\n",
    "                model.add(Dense(unit, activation='relu', input_shape=(runs_disease_x_train[(run, disease)].shape[1],)))\n",
    "                model.add(BatchNormalization())\n",
    "                model.add(Dense(unit, activation='relu'))\n",
    "                model.add(BatchNormalization())\n",
    "                model.add(Dense(runs_disease_n_intervals[(run, disease)], activation='sigmoid'))\n",
    "                adam = optimizers.Adam(lr=learning_rate)\n",
    "                model.compile(loss=surv_likelihood(runs_disease_n_intervals[(run, disease)]), optimizer=adam)\n",
    "                while True:\n",
    "                    model.fit(runs_disease_x_train[(run, disease)], runs_disease_y_train[(run, disease)], batch_size=256, epochs=1, shuffle=True, verbose=0)\n",
    "                    loss = model.evaluate(runs_disease_x_val[(run, disease)], runs_disease_y_val[(run, disease)], verbose=0)\n",
    "                    if loss < min_loss:\n",
    "                        num_epochs += 1\n",
    "                        min_loss = loss\n",
    "                        num_patience = 0\n",
    "                    else:\n",
    "                        if num_patience < patience:\n",
    "                            num_epochs += 1\n",
    "                            num_patience += 1\n",
    "                        else:\n",
    "                            break\n",
    "                runs_disease_baseline_errors[(run, (disease, learning_rate, num_epochs - patience, unit))] = min_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train each baseline model\n",
    "\n",
    "runs_disease_baseline_models = {}\n",
    "for run in range(10):\n",
    "    for disease in [item[1] for item in runs_disease_x_train.keys() if item[0] == run]:\n",
    "        this_disease_runs = [x for x in runs_disease_baseline_errors.keys() if disease == x[1][0] and run == x[0]]\n",
    "        lowest_error = 100000\n",
    "        best_run = None\n",
    "        for disease_run in this_disease_runs:\n",
    "            if runs_disease_baseline_errors[disease_run] < lowest_error:\n",
    "                lowest_error = runs_disease_baseline_errors[disease_run]\n",
    "                best_run = disease_run\n",
    "        print(best_run, runs_disease_baseline_errors[best_run])\n",
    "        total_x = np.vstack((runs_disease_x_train[(run, disease)], runs_disease_x_val[(run, disease)]))\n",
    "        total_y = np.vstack((runs_disease_y_train[(run, disease)], runs_disease_y_val[(run, disease)]))\n",
    "        total_e = np.hstack((runs_disease_e_train[(run, disease)], runs_disease_e_val[(run, disease)]))\n",
    "        model = Sequential()\n",
    "        model.add(Dense(best_run[1][3], activation='relu', input_shape=(total_x.shape[1],)))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Dense(best_run[1][3], activation='relu'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Dense(runs_disease_n_intervals[(run, disease)], activation='sigmoid'))\n",
    "        adam = optimizers.Adam(lr=best_run[1][1])\n",
    "        model.compile(loss=surv_likelihood(runs_disease_n_intervals[(run, disease)]), optimizer=adam)\n",
    "        model.fit(total_x, total_y, batch_size=256, epochs=best_run[1][2], shuffle=True)\n",
    "        runs_disease_baseline_models[(run, disease)] = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Test each baseline model\n",
    "\n",
    "runs_disease_baseline_loss = {}\n",
    "for run in range(10):\n",
    "    for disease in [item[1] for item in runs_disease_x_train.keys() if item[0] == run]:\n",
    "        test_loss = runs_disease_baseline_models[(run, disease)].evaluate(runs_disease_x_test[(run, disease)], runs_disease_y_test[(run, disease)])\n",
    "        runs_disease_baseline_loss[(run, disease)] = test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot loss differences for each disease\n",
    "\n",
    "disease_diff = {}\n",
    "for key in runs_disease_trained_global_loss.keys():\n",
    "    diff = runs_disease_baseline_loss[key] - runs_disease_trained_global_loss[key]\n",
    "    if key[1] not in disease_diff.keys():\n",
    "        disease_diff[key[1]] = []\n",
    "        disease_diff[key[1]].append(diff)\n",
    "    else:\n",
    "        disease_diff[key[1]].append(diff)\n",
    "        \n",
    "import plotly.plotly as py\n",
    "import plotly.graph_objs as go\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "data = [go.Box(y = disease_diff[x], name=x) for x in disease_diff.keys()]\n",
    "py.iplot(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature importance\n",
    "# Start by getting original losses before shuffling\n",
    "\n",
    "original_transfer_losses = {}\n",
    "for run in range(10):\n",
    "    for disease in [item[1] for item in runs_disease_x_train.keys() if item[0] == run]:\n",
    "        total_x = np.vstack((runs_disease_x_train[(run, disease)], runs_disease_x_val[(run, disease)]))\n",
    "        total_y = np.vstack((runs_disease_y_train[(run, disease)], runs_disease_y_val[(run, disease)]))\n",
    "        model = runs_disease_trained_global_models[(run, disease)]\n",
    "        original_transfer_loss = model.evaluate(total_x, total_y)\n",
    "        original_transfer_losses[(run, disease)] = original_transfer_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do permutation feature importance\n",
    "\n",
    "shuffled_losses = {}\n",
    "for run in range(10):\n",
    "    for disease in [item[1] for item in runs_disease_x_train.keys() if item[0] == run]:\n",
    "        total_x = np.vstack((runs_disease_x_train[(run, disease)], runs_disease_x_val[(run, disease)]))\n",
    "        total_y = np.vstack((runs_disease_y_train[(run, disease)], runs_disease_y_val[(run, disease)]))\n",
    "        model = runs_disease_trained_global_models[(run, disease)]\n",
    "        idx_new_loss = {}\n",
    "        for col_idx in range(runs_disease_x_train[(run, disease)].shape[1]):\n",
    "            print(run, disease, col_idx)\n",
    "            x_train_copy = total_x.copy()\n",
    "            np.random.shuffle(x_train_copy[:,col_idx])\n",
    "            new_loss = model.evaluate(x_train_copy, total_y)\n",
    "            idx_new_loss[col_idx] = new_loss\n",
    "        shuffled_losses[(run, disease)] = idx_new_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get loss diff for each disease run\n",
    "\n",
    "runs_disease_loss_diff = {}\n",
    "for run in range(10):\n",
    "    for disease in [item[1] for item in runs_disease_x_train.keys() if item[0] == run]:\n",
    "        idx_loss_diff = {}\n",
    "        orig_loss = original_transfer_losses[(run, disease)]\n",
    "        for col_idx in range(runs_disease_x_train[(run, disease)].shape[1]):\n",
    "            idx_loss_diff[col_idx] = shuffled_losses[(run, disease)][col_idx] - orig_loss\n",
    "        runs_disease_loss_diff[(run, disease)] = idx_loss_diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get indices of features which might have been removed for having all 0s\n",
    "\n",
    "df = pd.read_csv('binding/all_features/all_features.csv')\n",
    "df = df.dropna(subset=['days_to_death', 'days_to_followup'], how='all')\n",
    "\n",
    "imp = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "x_mut = np.asarray([[float(y) if y != 'NA' else np.nan for y in x[1:-1].split(\", \")] for x in list(df['mut_features'])])\n",
    "x_exp = np.asarray([[float(y) if y != 'NA' else np.nan for y in x[1:-1].split(\", \")] for x in list(df['exp_features'])])\n",
    "mut_load = np.sum(x_mut, axis=1).reshape(df.shape[0], 1)\n",
    "x = np.hstack((x_exp, mut_load))\n",
    "x = np.hstack((x, np.expand_dims(np.asarray(df['days_to_birth']), axis=1)))\n",
    "x = np.where(np.isnan(x), ma.array(x, mask=np.isnan(x)).mean(axis=0), x)\n",
    "imp.fit(x)\n",
    "x = imp.transform(x)\n",
    "scaler = preprocessing.StandardScaler().fit(x)\n",
    "\n",
    "disease_idx = disease_cols.index('disease_code_' + disease)\n",
    "\n",
    "df = df.sample(frac=1)\n",
    "train_size = int(0.75 * df.shape[0])\n",
    "test_size = df.shape[0] - train_size\n",
    "train = df.head(train_size)\n",
    "validate = df.tail(test_size)\n",
    "output_cols = ['days_to_death', 'days_to_followup']\n",
    "dtb_train = np.expand_dims(np.asarray(train['days_to_birth']), axis=1)\n",
    "mut_train = np.asarray([[float(y) if y != 'NA' else np.nan for y in x[1:-1].split(\", \")] for x in list(train['mut_features'])])\n",
    "mut_train_load = np.sum(mut_train, axis=1).reshape(train.shape[0], 1)\n",
    "exp_train = np.asarray([[float(y) if y != 'NA' else np.nan for y in x[1:-1].split(\", \")] for x in list(train['exp_features'])])\n",
    "print(exp_train.shape)\n",
    "x_train = np.hstack((exp_train, mut_train_load))\n",
    "print(x_train.shape)\n",
    "x_train = np.hstack((x_train, dtb_train))\n",
    "print(x_train.shape)\n",
    "x_train = imp.transform(x_train)\n",
    "x_train = scaler.transform(x_train)\n",
    "print(\"Indices deleted\", np.where(~x_train.any(axis=0))[0])\n",
    "x_train = x_train[:, ~np.all(x_train == 0, axis=0)]\n",
    "print(x_train.shape)\n",
    "x_train = np.hstack((x_train, mut_train))\n",
    "print(x_train.shape)\n",
    "zero_mat = np.zeros((x_train.shape[0], len(disease_cols)))\n",
    "zero_mat[:, disease_idx] = np.ones(x_train.shape[0])\n",
    "x_train = np.hstack((x_train, zero_mat))\n",
    "print(x_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get corresponding feature names for each index\n",
    "# Feat index 117 always removed when removing features with all 0s\n",
    "\n",
    "with open('binding/exp_order_list.txt', 'r') as f:\n",
    "    exp_feats = [line.strip() + '_exp' for line in f]\n",
    "    \n",
    "with open('binding/mut_order_list.txt', 'r') as f:\n",
    "    mut_feats = [line.strip() + '_mut' for line in f]\n",
    "    \n",
    "all_feats = exp_feats + ['mut_load', 'days_to_birth'] + mut_feats + disease_cols\n",
    "del all_feats[117]\n",
    "\n",
    "idx_feat = dict(zip(list(range(len(all_feats))), all_feats))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature importance for each specific disease\n",
    "\n",
    "total_feat_importance_dfs = []\n",
    "feat_size = len(list(runs_disease_loss_diff[(0, 'Liver Hepatocellular Carcinoma')].keys()))\n",
    "uniq_diseases = list(set([x[1] for x in list(runs_disease_loss_diff.keys())]))\n",
    "for disease in uniq_diseases:\n",
    "    disease_feature_loss_changes = {}\n",
    "    disease_dicts = list({k:v for (k,v) in runs_disease_loss_diff.items() if k[1] == disease}.values())\n",
    "    for i in range(feat_size):\n",
    "        disease_feature_loss_changes[i] = []\n",
    "    for disease_dict in disease_dicts:\n",
    "        for i in range(feat_size):\n",
    "            disease_feature_loss_changes[i].append(disease_dict[i])\n",
    "    avg_disease_feature_loss_changes = {k:np.mean(v) for (k,v) in disease_feature_loss_changes.items()}\n",
    "    avg_disease_named_feature_loss_changes = {idx_feat[k]:v for (k,v) in avg_disease_feature_loss_changes.items()}\n",
    "    feat_importance_df = pd.DataFrame.from_dict(avg_disease_named_feature_loss_changes, orient='index', columns=['importance (shuffled_loss - orig_loss)'])\n",
    "    feat_importance_df['feature'] = feat_importance_df.index\n",
    "    feat_importance_df = feat_importance_df.sort_values(by='importance (shuffled_loss - orig_loss)', ascending=False)\n",
    "    feat_importance_df['disease'] = disease\n",
    "    total_feat_importance_dfs.append(feat_importance_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "total_feat_importance_df = pd.concat(total_feat_importance_dfs)\n",
    "total_feat_importance_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_feat_importance_df.to_csv('binding/ind_disease_transfer_feature_importance.csv', index=False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
